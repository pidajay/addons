{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/training_gradient_checkpointing\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/training_gradient_checkpointing.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/training_gradient_checkpointing.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "      <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/docs/tutorials/training_gradient_checkpointing.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is gradient checkpointing?\n",
    "\n",
    "Gradient checkpointing enables users to train large models with relatively small memory resources. Large models could refer to \n",
    "1. Models with large variables i.e weight matrices. As a consequence such models have correspondingly large gradients and optimizer states. The activations (intermediate outputs from the model layers) tend to be relatively small (depends on the batch size). Typically fully connected networks and RNNs fall under this category.\n",
    "2. Models with small weights but large activations. CNNs and transformers tend to fall under this category.\n",
    "\n",
    "It is important to note that gradient checkpointing is meant to help with models of type 2. Models of type 1 do not stand to gain much benefit.\n",
    "\n",
    "### How is it implemented?\n",
    "\n",
    "1. *Recompute step* - This is the core step in gradient checkpointing. Recompute allows one to calculate the  forward activations while doing the backward pass. Therefore there is no need to store intermediate activations during forward pass, thus saving memory. But during the backward pass, for every layer the forward  activations need to be recomputed starting from the first layer, thus increasing time to train.\n",
    "2. *Checkpoint step* - This step allows one to designate (either manually or automatically) certain layers in the model as ‘checkpoint layers’ whose activations will be persisted in memory during the forward pass.. This essentially allows the user to balance the memory vs time tradeoff. By checkpointing certain layers, one just has to recompute new activations from the last checkpoint layer thereby improving training times.\n",
    "\n",
    "This tutorial demonstrates how to use the recompute step implemented in TF add ons to train a TF Keras model. Note that at this time the implementation works only in eager mode for TF 2.x and only for sequential networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtylpxOmceaC"
   },
   "outputs": [],
   "source": [
    "# Build the model. This is a sample large sequential cnn model.\n",
    "def get_cnn_model(img_dim, n_channels):\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Reshape(\n",
    "        target_shape=[img_dim, img_dim, n_channels],\n",
    "        input_shape=(img_dim, img_dim, n_channels)),\n",
    "    layers.Conv2D(10, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(20, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(60, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(20, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((1, 1), padding='same'),\n",
    "    layers.Conv2D(20, 5, padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(10)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "def compute_loss(logits, labels):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this how you wrap your model with the 'recompute decorator'\n",
    "@tfa.training.recompute_sequential\n",
    "def model_fn(model, x):\n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform training with dummy inputs\n",
    "def train():\n",
    "    img_dim = 256\n",
    "    n_channels = 1\n",
    "    bs = 16\n",
    "    x = tf.ones([bs,img_dim,img_dim,1])\n",
    "    y = tf.ones([bs], dtype=tf.int64)\n",
    "    model = get_cnn_model(img_dim, n_channels)\n",
    "    optimizer = optimizers.SGD()\n",
    "    for _ in range(5):\n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model_fn(model, x, _watch_vars=model.trainable_variables)\n",
    "            # To train without recompute, uncomment this line and remove the recompute_sequential decorator\n",
    "            #logits = model_fn(model, x)\n",
    "            loss  = compute_loss(logits, y)\n",
    "            print('loss', loss) \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        del grads \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tf.Tensor(2.3119702, shape=(), dtype=float32)\n",
      "loss tf.Tensor(2.2820306, shape=(), dtype=float32)\n",
      "loss tf.Tensor(2.22289, shape=(), dtype=float32)\n",
      "loss tf.Tensor(1.9491141, shape=(), dtype=float32)\n",
      "loss tf.Tensor(0.48555315, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# call the train function\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "_template.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.5.2 64-bit",
   "language": "python",
   "name": "python35264bitef1228cac4ef4242b56659dab072a033"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
